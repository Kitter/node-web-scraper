# 基于node.js的爬虫框架

##介绍
借鉴自己在公司实习时的爬虫项目经历，自己也开发了一个简易的node.js爬虫框架。框架主要可以用于网站文本信息的抓取，如视频网站中，所有视频标题、概况等介绍信息；微博账号中，大量用户发表的微博内容；新闻网站的多篇报道等等。框架支持多进程同时爬取多个网站，每个进程中又支持多个worker同时爬取一个网站下的多个url，最终的性能瓶颈主要在网络带宽。

##工作原理
在给出一个主入口url后，框架要爬取页面的信息，主要有两类：一类是我们真正需要的信息，如微博内容，视频介绍，新闻文章等；另一类则是下一页/加载更多/微博内容中@的用户/相关文章等，它们的url地址。把这两样信息放在两个数据结构返回，框架把内容信息，写出到txt文件中，而把新的url地址（通常有多个），全加入到框架的工作队列中，供下次爬取，并不断重复循环。

把每个页面想象成，一张图中的一个点，最终就是利用类似BFS宽度优先搜索的思路，对整个域名下的大量页面都进行爬取(需要做去重处理)，并把大量的信息爬取到本地。



##使用方法
###一、网站解析
对每个想爬取的网站，需要在website文件夹中，编写对应的爬虫解析。具体的方法如下：

+ 1）建立一个同根域名子文件夹（如：www.baidu.com）。
+ 2）在文件夹中添加 '_.js'，作为路由文件，用于建立网站子页面，与对应处理解析文件的映射关系。
+ 3）编写页面解析文件。先介绍一下框架的思路，
输出的数据,在每个解析器中的'output_data'子文件夹中。其中一个文件为所有爬取过的url地址，另一个为所有播放页面爬取回来的视频信息。

============

在config/config.txt中配置worker数目，超时时间，与入口任务数组。

###启动方式为 'node main.js' 


